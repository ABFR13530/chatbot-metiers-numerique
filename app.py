import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import tempfile

# â”€â”€ Configuration de la page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Conseiller MÃ©tiers du NumÃ©rique",
    page_icon="ğŸ’»",
    layout="centered"
)

# â”€â”€ CSS personnalisÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    .main { background-color: #f8f9fa; }
    .stChatMessage { border-radius: 12px; margin-bottom: 8px; }
    .profile-badge {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 13px;
        font-weight: 600;
        margin-bottom: 12px;
    }
    .badge-scolaire { background-color: #d0f0c0; color: #2d6a2d; }
    .badge-emploi { background-color: #cce5ff; color: #004085; }
    .badge-reconversion { background-color: #fff3cd; color: #856404; }
    .badge-indefini { background-color: #e2e3e5; color: #383d41; }
    h1 { color: #1a1a2e; }
</style>
""", unsafe_allow_html=True)

# â”€â”€ Profils disponibles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROFILS = {
    "scolaire": {
        "label": "ğŸ“ Ã‰lÃ¨ve / Ã‰tudiant",
        "badge_class": "badge-scolaire",
        "system": """Tu es un conseiller expert en orientation scolaire vers les mÃ©tiers du numÃ©rique.
Tu t'adresses Ã  des Ã©lÃ¨ves et Ã©tudiants. Ton ton est encourageant, accessible et motivant.
Tu proposes des pistes de formations, de diplÃ´mes, et d'expÃ©riences pratiques.
RÃ©ponds toujours en franÃ§ais."""
    },
    "emploi": {
        "label": "ğŸ” Demandeur d'emploi",
        "badge_class": "badge-emploi",
        "system": """Tu es un conseiller emploi spÃ©cialisÃ© dans les mÃ©tiers du numÃ©rique.
Tu aides les personnes en recherche d'emploi Ã  identifier les mÃ©tiers porteurs, les compÃ©tences recherchÃ©es et les formations rapides.
Ton ton est professionnel, pragmatique et bienveillant.
RÃ©ponds toujours en franÃ§ais."""
    },
    "reconversion": {
        "label": "ğŸ”„ Cadre en reconversion",
        "badge_class": "badge-reconversion",
        "system": """Tu es un coach expert en reconversion professionnelle vers le numÃ©rique.
Tu t'adresses Ã  des cadres et professionnels expÃ©rimentÃ©s. Tu valorises leurs compÃ©tences transverses.
Tu proposes des passerelles mÃ©tier rÃ©alistes et des plans de transition concrets.
RÃ©ponds toujours en franÃ§ais."""
    },
    "indefini": {
        "label": "â“ Profil Ã  dÃ©finir",
        "badge_class": "badge-indefini",
        "system": """Tu es un conseiller en orientation gÃ©nÃ©rale sur les mÃ©tiers du numÃ©rique.
Tu poses des questions pour mieux comprendre la situation de l'utilisateur avant de l'orienter.
Sois curieux, bienveillant et progressif dans tes questions.
RÃ©ponds toujours en franÃ§ais."""
    }
}

PROMPT_DETECTION_PROFIL = """
Analyse ce message et dÃ©termine le profil de l'utilisateur parmi : scolaire, emploi, reconversion, indefini.
- scolaire : Ã©lÃ¨ve, Ã©tudiant, lycÃ©en, en Ã©tudes, orientation scolaire
- emploi : demandeur d'emploi, cherche du travail, chÃ´mage, reconversion rapide
- reconversion : cadre, manager, professionnel expÃ©rimentÃ© souhaitant changer de mÃ©tier
- indefini : pas assez d'Ã©lÃ©ments

Message : "{message}"

RÃ©ponds uniquement avec un mot parmi : scolaire, emploi, reconversion, indefini
"""

# â”€â”€ Fonctions utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_profil(message: str, llm) -> str:
    """DÃ©tecte le profil utilisateur via le LLM."""
    try:
        prompt = PROMPT_DETECTION_PROFIL.format(message=message)
        response = llm.invoke(prompt)
        profil = response.content.strip().lower()
        if profil in PROFILS:
            return profil
        # DÃ©tection par mots-clÃ©s de secours
        msg = message.lower()
        if any(k in msg for k in ["Ã©tudiant", "lycÃ©e", "Ã©cole", "universitÃ©", "bac", "Ã©tudes"]):
            return "scolaire"
        if any(k in msg for k in ["emploi", "travail", "chÃ´mage", "recrutement", "cv"]):
            return "emploi"
        if any(k in msg for k in ["reconversion", "cadre", "manager", "expÃ©rience", "changer"]):
            return "reconversion"
    except:
        pass
    return "indefini"


def load_documents(uploaded_files):
    """Charge et dÃ©coupe les documents PDF/Word uploadÃ©s."""
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

    for uploaded_file in uploaded_files:
        suffix = "." + uploaded_file.name.split(".")[-1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name

        try:
            if suffix == ".pdf":
                loader = PyMuPDFLoader(tmp_path)
            elif suffix in [".docx", ".doc"]:
                loader = Docx2txtLoader(tmp_path)
            else:
                continue
            raw_docs = loader.load()
            chunks = splitter.split_documents(raw_docs)
            docs.extend(chunks)
        except Exception as e:
            st.warning(f"Erreur lors du chargement de {uploaded_file.name} : {e}")
        finally:
            os.unlink(tmp_path)

    return docs


def build_vectorstore(docs, embeddings):
    """Construit la base vectorielle ChromaDB."""
    return Chroma.from_documents(docs, embeddings)


def build_chain(vectorstore, llm, profil_key: str):
    """Construit la chaÃ®ne RAG conversationnelle."""
    profil = PROFILS[profil_key]

    prompt_template = profil["system"] + """

Utilise les extraits de documents suivants pour rÃ©pondre Ã  la question.
Si les documents ne contiennent pas la rÃ©ponse, utilise tes connaissances gÃ©nÃ©rales sur les mÃ©tiers du numÃ©rique.

Contexte documentaire :
{context}

Historique de la conversation :
{chat_history}

Question : {question}

RÃ©ponse :"""

    qa_prompt = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template=prompt_template
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": qa_prompt},
        return_source_documents=False,
        verbose=False
    )
    return chain


# â”€â”€ Initialisation session state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_session():
    defaults = {
        "messages": [],
        "profil": None,
        "chain": None,
        "vectorstore": None,
        "docs_loaded": False,
        "api_key_ok": False,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


# â”€â”€ Interface principale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    init_session()

    st.title("ğŸ’» Conseiller MÃ©tiers du NumÃ©rique")
    st.caption("Un assistant IA pour explorer et choisir votre voie dans le numÃ©rique")

    # â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.sidebar:
        st.header("âš™ï¸ Configuration")

        # ClÃ© API Gemini
        api_key = st.text_input(
            "ğŸ”‘ ClÃ© API Google Gemini",
            type="password",
            placeholder="AIza...",
            help="Obtenez votre clÃ© gratuite sur https://aistudio.google.com"
        )

        if api_key:
            os.environ["GOOGLE_API_KEY"] = api_key
            st.session_state.api_key_ok = True
            st.success("âœ… ClÃ© API configurÃ©e")

        st.divider()

        # Upload documents
        st.subheader("ğŸ“„ Vos documents")
        uploaded_files = st.file_uploader(
            "Ajoutez vos PDF / Word",
            type=["pdf", "docx", "doc"],
            accept_multiple_files=True,
            help="Ces documents enrichiront les rÃ©ponses du conseiller"
        )

        if uploaded_files and st.session_state.api_key_ok:
            if st.button("ğŸ“¥ Charger les documents", use_container_width=True):
                with st.spinner("Indexation en cours..."):
                    try:
                        llm = ChatGoogleGenerativeAI(
                            model="gemini-2.0-flash",
                            temperature=0.3,
                            google_api_key=api_key
                        )
                        embeddings = GoogleGenerativeAIEmbeddings(
                            model="models/embedding-001",
                            google_api_key=api_key
                        )
                        docs = load_documents(uploaded_files)
                        if docs:
                            st.session_state.vectorstore = build_vectorstore(docs, embeddings)
                            st.session_state.docs_loaded = True
                            st.session_state.llm = llm
                            st.success(f"âœ… {len(docs)} extraits indexÃ©s !")
                        else:
                            st.error("Aucun document valide trouvÃ©.")
                    except Exception as e:
                        st.error(f"Erreur : {e}")

        st.divider()

        # Profil manuel
        st.subheader("ğŸ‘¤ Profil dÃ©tectÃ©")
        if st.session_state.profil:
            p = PROFILS[st.session_state.profil]
            st.markdown(
                f'<span class="profile-badge {p["badge_class"]}">{p["label"]}</span>',
                unsafe_allow_html=True
            )
        else:
            st.info("Profil auto-dÃ©tectÃ© Ã  la premiÃ¨re question")

        if st.button("ğŸ”„ RÃ©initialiser la conversation", use_container_width=True):
            st.session_state.messages = []
            st.session_state.profil = None
            st.session_state.chain = None
            st.rerun()

        st.divider()
        st.caption("POC â€” Chatbot MÃ©tiers du NumÃ©rique\nPowered by Gemini Flash 2.0 + RAG")

    # â”€â”€ Zone principale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not st.session_state.api_key_ok:
        st.info("ğŸ‘ˆ Commencez par entrer votre clÃ© API Gemini dans le panneau de gauche.")
        st.markdown("""
        **Comment obtenir une clÃ© gratuite ?**
        1. Rendez-vous sur [Google AI Studio](https://aistudio.google.com)
        2. Connectez-vous avec votre compte Google
        3. Cliquez sur **"Get API Key"**
        4. Copiez-collez la clÃ© ici
        """)
        return

    # Message d'accueil
    if not st.session_state.messages:
        with st.chat_message("assistant"):
            st.markdown("""
Bonjour ! ğŸ‘‹ Je suis votre **conseiller en mÃ©tiers du numÃ©rique**.

Je suis lÃ  pour vous aider Ã  :
- ğŸ“ DÃ©couvrir les mÃ©tiers du numÃ©rique selon votre profil
- ğŸ’¡ Obtenir des conseils personnalisÃ©s d'orientation
- ğŸš€ Construire votre projet professionnel dans le secteur tech

**Dites-moi qui vous Ãªtes et ce que vous recherchez !**
*(exemple : "Je suis Ã©tudiant en terminale et je cherche ma voie dans l'informatique")*
            """)

    # Affichage historique
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Input utilisateur
    if user_input := st.chat_input("Posez votre question..."):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            with st.spinner("RÃ©flexion en cours..."):
                try:
                    llm = ChatGoogleGenerativeAI(
                        model="gemini-2.0-flash",
                        temperature=0.3,
                        google_api_key=api_key
                    )

                    # DÃ©tection du profil au premier message
                    if not st.session_state.profil:
                        profil = detect_profil(user_input, llm)
                        st.session_state.profil = profil
                        p = PROFILS[profil]
                        st.markdown(
                            f'<span class="profile-badge {p["badge_class"]}">Profil dÃ©tectÃ© : {p["label"]}</span>',
                            unsafe_allow_html=True
                        )

                    # Construction/rÃ©cupÃ©ration de la chaÃ®ne RAG
                    if st.session_state.chain is None:
                        if st.session_state.vectorstore:
                            st.session_state.chain = build_chain(
                                st.session_state.vectorstore,
                                llm,
                                st.session_state.profil
                            )
                        else:
                            # Sans documents : LLM seul avec prompt profil
                            profil_info = PROFILS[st.session_state.profil]
                            system = profil_info["system"]
                            history = "\n".join([
                                f"{m['role'].capitalize()} : {m['content']}"
                                for m in st.session_state.messages[:-1]
                            ])
                            full_prompt = f"{system}\n\nHistorique :\n{history}\n\nQuestion : {user_input}\n\nRÃ©ponse :"
                            response = llm.invoke(full_prompt)
                            answer = response.content
                            st.markdown(answer)
                            st.session_state.messages.append({"role": "assistant", "content": answer})
                            return

                    # Appel RAG
                    result = st.session_state.chain({"question": user_input})
                    answer = result["answer"]
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})

                except Exception as e:
                    err_msg = f"âŒ Erreur : {str(e)}"
                    st.error(err_msg)
                    st.session_state.messages.append({"role": "assistant", "content": err_msg})


if __name__ == "__main__":
    main()
